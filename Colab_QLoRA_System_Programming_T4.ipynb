{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6d48f7d1",
      "metadata": {
        "id": "6d48f7d1"
      },
      "source": [
        "# ðŸ”§ Practical Blueprint: QLoRA Fine-Tuning for C & Linux System Programming (T4-ready)\n",
        "\n",
        "This Colab-ready notebook fine-tunes a 7B instruct model using **QLoRA** on a **T4 (16GB) GPU**.\n",
        "\n",
        "## What you get\n",
        "- QLoRA training on a small example dataset (you can replace with your own)\n",
        "- Domain-focused formatting for **C & Linux system programming**\n",
        "- Safe defaults for T4: 4-bit quantization, small per-device batch, gradient accumulation\n",
        "- Quick evaluation and (optional) compile-check of generated C code\n",
        "\n",
        "**Tip:** For real training, plug in your larger dataset and extend training time/epochs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f580fa70",
      "metadata": {
        "id": "f580fa70"
      },
      "source": [
        "## 0) Runtime & GPU check\n",
        "- In Colab: `Runtime â†’ Change runtime type â†’ GPU (T4)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07d1f3c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07d1f3c0",
        "outputId": "cf574a23-65e7-4abb-813f-ed8bb3b057f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "Torch: 2.8.0+cu126\n",
            "Python: 3.12.11\n"
          ]
        }
      ],
      "source": [
        "import torch, platform\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "print('GPU:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU')\n",
        "print('Torch:', torch.__version__)\n",
        "print('Python:', platform.python_version())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c20fa55",
      "metadata": {
        "id": "8c20fa55"
      },
      "source": [
        "## 1) Install dependencies\n",
        "We pin versions known to work well on T4 + Colab."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Inspect environment (optional) ---\n",
        "!nvidia-smi\n",
        "!pip install -q -U transformers accelerate git+https://github.com/huggingface/peft.git\n",
        "!pip install -q datasets bitsandbytes einops wandb\n",
        "!pip install \"trl==0.9.6\" -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SxVGTvO2Jyi",
        "outputId": "7fba808a-7d66-41f5-822c-46307d881d45"
      },
      "id": "1SxVGTvO2Jyi",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Sep 27 19:17:45 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ff45c4b",
      "metadata": {
        "id": "8ff45c4b"
      },
      "source": [
        "## 2) Configuration\n",
        "Choose a base model. Defaults to **Mistral 7B Instruct** (permissive license)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "82691676",
      "metadata": {
        "id": "82691676",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ce48bf4-5a45-47ec-f09d-486173c60b94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trl: 0.9.6\n"
          ]
        }
      ],
      "source": [
        "import trl, transformers\n",
        "import importlib, pkgutil\n",
        "print(\"trl:\", trl.__version__)\n",
        "BASE_MODEL = 'mistralai/Mistral-7B-Instruct-v0.2'  # or mistralai/Mistral-7B-Instruct-v0.3 if available\n",
        "OUTPUT_DIR = '/content/qlora-mistral-sysprog'\n",
        "USE_4BIT = True  # QLoRA\n",
        "SEQ_LEN = 2048   # keep <= 2048 for T4 comfort\n",
        "EPOCHS = 1       # increase for real training\n",
        "LR = 2e-4\n",
        "BATCH_SIZE = 1\n",
        "GRAD_ACCUM = 16  # effective batch = 16\n",
        "WARMUP_RATIO = 0.05\n",
        "SAVE_STEPS = 200\n",
        "LOG_STEPS = 10\n",
        "VAL_SET_SIZE = 0  # set >0 if you add a validation split"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "315d4967",
      "metadata": {
        "id": "315d4967"
      },
      "source": [
        "## 3) Prepare a **sample** domain dataset (replace with your own)\n",
        "Format: instruction-style samples targeting C & Linux system topics. We create a tiny JSONL to demo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c05a036e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c05a036e",
        "outputId": "410abb6c-a19c-4465-d710-ffa30102258b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved /content/data/sysprog_train.jsonl with 3 examples\n"
          ]
        }
      ],
      "source": [
        "import json, os\n",
        "os.makedirs('/content/data', exist_ok=True)\n",
        "samples = [\n",
        "  {\n",
        "    'instruction': 'Explain the difference between fork() and vfork() with a small C example.',\n",
        "    'input': '',\n",
        "    'output': (\n",
        "      'fork() creates a child with a copy-on-write address space; both parent and child may run concurrently. '\n",
        "      'vfork() guarantees the child runs first and shares the address space until exec/exit; parent is suspended.\\n\\n'\n",
        "      '```c\\n#include <stdio.h>\\n#include <sys/types.h>\\n#include <unistd.h>\\nint main(){ pid_t p=fork(); if(p==0){printf(\"child\\\\n\");} else {printf(\"parent\\\\n\");} }\\n```'\n",
        "    )\n",
        "  },\n",
        "  {\n",
        "    'instruction': 'Write a C function that safely reads a file and prints its content using POSIX APIs.',\n",
        "    'input': '',\n",
        "    'output': (\n",
        "      '```c\\n#define _XOPEN_SOURCE 700\\n#include <fcntl.h>\\n#include <unistd.h>\\n#include <stdio.h>\\n#include <errno.h>\\n#include <string.h>\\n'\n",
        "      'void print_file(const char *path){ int fd=open(path,O_RDONLY); if(fd<0){perror(\"open\");return;} '\n",
        "      'char buf[4096]; ssize_t n; while((n=read(fd,buf,sizeof(buf)))>0){ if(write(STDOUT_FILENO,buf,n)<0){perror(\"write\");break;} } '\n",
        "      'if(n<0) perror(\"read\"); if(close(fd)<0) perror(\"close\"); }\\n```'\n",
        "    )\n",
        "  },\n",
        "  {\n",
        "    'instruction': 'Given this segfaulting code, identify the bug and provide a fix. Code: int *p=NULL; *p=42; explain.',\n",
        "    'input': '',\n",
        "    'output': 'Dereferencing a NULL pointer causes a segmentation fault. Allocate memory or point to a valid object before dereferencing.'\n",
        "  }\n",
        "]\n",
        "with open('/content/data/sysprog_train.jsonl', 'w') as f:\n",
        "    for s in samples:\n",
        "        f.write(json.dumps(s)+'\\n')\n",
        "print('Saved /content/data/sysprog_train.jsonl with', len(samples), 'examples')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "007f0323",
      "metadata": {
        "id": "007f0323"
      },
      "source": [
        "## 4) Dataset loader\n",
        "Converts instruction-style JSONL into a chat template compatible with the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5eeb30f8",
      "metadata": {
        "id": "5eeb30f8"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
        "ds = load_dataset(\"json\", data_files={\n",
        "    \"train\": \"/content/sample_data/sysprog_train_large.jsonl\",\n",
        "    \"valid\": \"/content/sample_data/sysprog_valid_small.jsonl\",\n",
        "})\n",
        "\n",
        "def to_chat_format(example):\n",
        "    user = example[\"instruction\"]\n",
        "    if example.get(\"input\"):\n",
        "        user += f\"\\n\\nInput:\\n{example['input']}\"\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": user},\n",
        "        {\"role\": \"assistant\", \"content\": example[\"output\"]},\n",
        "    ]\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
        "    return {\"text\": text}\n",
        "\n",
        "train_ds = ds[\"train\"].map(to_chat_format, remove_columns=[c for c in ds[\"train\"].column_names if c != \"text\"])\n",
        "valid_ds = ds[\"valid\"].map(to_chat_format, remove_columns=[c for c in ds[\"valid\"].column_names if c != \"text\"])\n",
        "\n",
        "# With TRL SFTTrainer, you can re-enable packing now that you have enough data:\n",
        "# SEQ_LEN = 1024 (or 1536 if memory allows)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fec7ab16",
      "metadata": {
        "id": "fec7ab16"
      },
      "source": [
        "## 5) QLoRA setup & tokenizer\n",
        "- Load 4-bit quantized base model weights (nf4) for training adapters\n",
        "- Use **fp16** compute on T4 (bf16 is not supported on T4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "91231afd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "51dc6f75423f49a3a582e828bafb73c3",
            "7ae052dd14db4509bc97c0410811709f",
            "ea9090d7558e464cad1cdecaf5cc924c",
            "2712d28d8a3f46e885f9f241f79cff48",
            "8bddd3e8dcf34c3fa44109af0b0e5307",
            "85da4ee55d6845fb96e75f60249d8d14",
            "0ee0f4965f0648ccb22ebe42f2005c7a",
            "5480ec1764e44750b8f2ed941af983e1",
            "2c1db38d1dd9456cac708e2221142824",
            "339083708a71403c86992adaf2ea48f7",
            "4bcb5850409343bcb997d62645d9d79d"
          ]
        },
        "id": "91231afd",
        "outputId": "8f2ae454-89a9-462e-83e7-9064fa008842"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51dc6f75423f49a3a582e828bafb73c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, TaskType\n",
        "import torch\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=USE_4BIT,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=bnb_config if USE_4BIT else None,\n",
        "    device_map='auto'\n",
        ")\n",
        "model.config.use_cache = False  # important for gradient checkpointing\n",
        "print('Model loaded.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28714890",
      "metadata": {
        "id": "28714890"
      },
      "source": [
        "## 6) Format dataset to chat prompts\n",
        "We wrap each instruction into an **assistant-style completion** prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "493b9f09",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "493b9f09",
        "outputId": "e0a170df-c59e-44e2-9658-7e1a8b33aee2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': '### Instruction:\\nProvide a minimal TCP server in C that listens on port 10000 and echoes one line.\\n\\n### Response:\\n```c\\n#include <stdio.h>\\n#include <sys/types.h>\\n#include <sys/socket.h>\\n#include <netinet/in.h>\\n#include <arpa/inet.h>\\n#include <unistd.h>\\n\\nint main(int argc, char **argv) {\\n    int s=socket(AF_INET,SOCK_STREAM,0); if(s<0){perror(\"socket\");return 1;}\\n    struct sockaddr_in a={0}; a.sin_family=AF_INET; a.sin_addr.s_addr=htonl(INADDR_ANY); a.sin_port=htons(10000);\\n    int on=1; setsockopt(s,SOL_SOCKET,SO_REUSEADDR,&on,sizeof(on));\\n    if(bind(s,(struct sockaddr*)&a,sizeof(a))<0){perror(\"bind\");return 1;} listen(s,1);\\n    int c=accept(s,NULL,NULL); if(c<0){perror(\"accept\");return 1;} char b[256]; ssize_t n=read(c,b,sizeof(b)); if(n>0) write(c,b,n);\\n    close(c); close(s); return 0;\\n}\\n```'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "def format_example(example):\n",
        "    instruction = example['instruction']\n",
        "    input_text = example.get('input','')\n",
        "    output_text = example['output']\n",
        "    if input_text:\n",
        "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n\"\n",
        "    else:\n",
        "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
        "    return {\n",
        "        'text': prompt + output_text\n",
        "    }\n",
        "\n",
        "train_ds = ds['train'].map(format_example)\n",
        "train_ds = train_ds.remove_columns([c for c in train_ds.column_names if c!='text'])\n",
        "train_ds = train_ds.shuffle(seed=42)\n",
        "train_ds[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3c6ad9e",
      "metadata": {
        "id": "e3c6ad9e"
      },
      "source": [
        "## 7) Trainer (TRL SFTTrainer with PEFT LoRA)\n",
        "We apply LoRA on key projection and MLP layers commonly used for Mistral/LLama."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from peft import LoraConfig, TaskType\n",
        "from peft import LoraConfig, get_peft_model"
      ],
      "metadata": {
        "id": "bCIs8ogHgKOe"
      },
      "id": "bCIs8ogHgKOe",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=['q_proj','k_proj','v_proj','o_proj','gate_proj','up_proj','down_proj'],\n",
        "    lora_dropout=0.05,\n",
        "    bias='none',\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM,\n",
        "    learning_rate=LR,\n",
        "    lr_scheduler_type='cosine',\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    logging_steps=LOG_STEPS,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    save_total_limit=2,\n",
        "    fp16=True, bf16=False,\n",
        "    max_grad_norm=1.0,\n",
        "    gradient_checkpointing=True,\n",
        "    optim='paged_adamw_8bit',\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,           # OK with TRL 0.9.6\n",
        "    train_dataset=train_ds,\n",
        "    peft_config=lora_config,\n",
        "    max_seq_length=SEQ_LEN,        # OK with TRL 0.9.6\n",
        "    dataset_text_field='text',\n",
        "    packing=True,                  # OK with TRL 0.9.6\n",
        "    args=training_args,\n",
        ")\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752,
          "referenced_widgets": [
            "0de12dfea8a443d79ec0b6059e4d9b86",
            "9f954891b9304605a66c75be93709ebc",
            "8991659f0eba4db9b320894c448792d8",
            "41f11733fe38456c9eb2a0800fd54eaa",
            "1c6f63c23c09488cad8ac0716126b750",
            "85f0c9e4e04741709d231ddf2bd1c5a5",
            "200bc99abf364fef9e74f16f9d39af18",
            "e5cdc67e161d4739bc7b2797dc147bc9",
            "427c04eb67ec4c9982034a7b705ff82d",
            "a869b29ac5764a618f84e096981beab2",
            "488eb2d472e04f0496634cc82d405aad"
          ]
        },
        "id": "zsLV2pRuqQIU",
        "outputId": "85887723-1e3a-44d9-a047-cd5d3fd57c95"
      },
      "id": "zsLV2pRuqQIU",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field, packing. Will not be supported from version '1.0.0'.\n",
            "\n",
            "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/sft_trainer.py:192: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0de12dfea8a443d79ec0b6059e4d9b86"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/sft_trainer.py:408: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/sft_trainer.py:413: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n",
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshwetabh-mmmec\u001b[0m (\u001b[33mshwetabh-mmmec-na\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250927_192716-aii9uock</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/shwetabh-mmmec-na/huggingface/runs/aii9uock' target=\"_blank\">polar-glade-3</a></strong> to <a href='https://wandb.ai/shwetabh-mmmec-na/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/shwetabh-mmmec-na/huggingface' target=\"_blank\">https://wandb.ai/shwetabh-mmmec-na/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/shwetabh-mmmec-na/huggingface/runs/aii9uock' target=\"_blank\">https://wandb.ai/shwetabh-mmmec-na/huggingface/runs/aii9uock</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 00:01, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1, training_loss=0.7722641825675964, metrics={'train_runtime': 91.7987, 'train_samples_per_second': 0.022, 'train_steps_per_second': 0.011, 'total_flos': 175782374670336.0, 'train_loss': 0.7722641825675964, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b298cb8d",
      "metadata": {
        "id": "b298cb8d"
      },
      "source": [
        "## 8) Save adapter & run quick inference\n",
        "We keep the base in 4-bit and use LoRA adapters for generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "c95a5a89",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6a2de94f4ab1472d8154b196b1bf574d",
            "0f358e597d784e75aabc83c17121027a",
            "5d8c46d42a174afa82a5328df18300da",
            "77a2651524484c879413d196d8a5b1dc",
            "131d78752814405383dfe693b43949ca",
            "d0c39a3dcdb944fdbd8f385fe7c42a0e",
            "f7ffef4bc8364330b9501ef5e36e1088",
            "997c61c43a624d9a977692b1b7a2d56b",
            "726a55c6e5d847d69eda8a8efc317b6b",
            "067e7895e5934164ba34ec3408acdf61",
            "f14d96ecefa14c1b8a02021ade99ea3e"
          ]
        },
        "id": "c95a5a89",
        "outputId": "081df19a-6f09-4558-c1bb-4bcb60645e0f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a2de94f4ab1472d8154b196b1bf574d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "To create a producer-consumer problem with a single-slot buffer using pthread condition variables, we'll create two threads: one for the producer and another for the consumer. The single-slot buffer will be implemented using a mutex and a boolean flag to indicate whether the buffer is full or empty.\n",
            "\n",
            "Here's the code:\n",
            "\n",
            "```c\n",
            "#include <pthread.h>\n",
            "#include <stdbool.h>\n",
            "#include <stdio.h>\n",
            "#include <stdlib.h>\n",
            "#include <unistd.h>\n",
            "\n",
            "#define BUFFER_SIZE 1\n",
            "\n",
            "pthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER;\n",
            "pthread_cond_t cond_full = PTHREAD_COND_INITIALIZER;\n",
            "pthread_cond_t cond_empty = PTHREAD_COND_INITIALIZER;\n",
            "bool buffer_full = false;\n",
            "\n",
            "void producer(void) {\n",
            "    int data = 0;\n",
            "\n",
            "    while (true) {\n",
            "        pthread_mutex_lock(&mutex);\n",
            "\n",
            "        // Wait if the buffer is full\n",
            "        while (buffer_full) {\n",
            "            pthread_cond_wait(&cond_empty, &mutex);\n",
            "        }\n",
            "\n",
            "        // Produce data and put it in the buffer\n",
            "        printf(\"Producer: Produced data: %d\\n\", data);\n",
            "        data = (data + 1) % 10;\n",
            "        buffer_full = true;\n",
            "\n",
            "        // Signal the consumer that data is available\n",
            "        pthread_cond_signal(&cond_full);\n",
            "\n",
            "        pthread_mutex_unlock(&mutex);\n",
            "\n",
            "        sleep(1);\n",
            "    }\n",
            "}\n",
            "\n",
            "void consumer(void) {\n",
            "    while (true) {\n",
            "        pthread_mutex_lock(&mutex);\n",
            "\n",
            "        // Wait if the buffer is empty\n",
            "        while (!buffer_full) {\n",
            "            pthread_cond_wait(&cond_full, &mutex);\n",
            "        }\n",
            "\n",
            "        // Consume data from the buffer\n",
            "        printf(\"Consumer: Consumed data: %d\\n\", data);\n",
            "        buffer_full = false;\n",
            "\n",
            "        // Signal the producer that the buffer is empty\n",
            "        pthread_cond_signal(&cond_empty);\n",
            "\n",
            "        pthread_mutex_unlock(&mutex);\n",
            "\n",
            "        sleep(1);\n",
            "    }\n",
            "}\n",
            "\n",
            "int main() {\n",
            "    pthread_t producer_thread, consumer_thread;\n",
            "\n",
            "    pthread_create(&producer_thread, NULL, &producer, NULL);\n",
            "    pthread_create(&consumer_thread, NULL, &consumer, NULL);\n",
            "\n",
            "    pthread_join(producer_thread, NULL);\n",
            "    pthread_join(consumer_thread, NULL);\n",
            "\n",
            "    return 0;\n",
            "}\n",
            "```\n",
            "\n",
            "In this example, the producer and consumer threads wait for each other using condition variables when the buffer is full or empty. The producer thread produces data and puts it in the buffer when it's empty, while the consumer thread consumes data from the buffer when it's full. The mutex is used to ensure that only one thread can access the buffer at a time.\n",
            "\n",
            "This code demonstrates the usage of pthread condition variables in a producer-consumer problem with a single-slot buffer.\n"
          ]
        }
      ],
      "source": [
        "# 0) Clean up RAM/VRAM from prior models\n",
        "import gc, torch\n",
        "try:\n",
        "    del model, trainer\n",
        "except NameError:\n",
        "    pass\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 1) Recreate base EXACTLY like training, but force all on GPU 0\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "\n",
        "assert torch.cuda.is_available(), \"No GPU detected. Change runtime to GPU.\"\n",
        "\n",
        "BASE_MODEL_NAME = BASE_MODEL  # e.g. \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,  # use torch.float16 if that's what you trained with\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, use_fast=True)\n",
        "\n",
        "# ðŸ”§ KEY FIX: keep ALL modules on a single GPU; 4-bit dispatch to CPU/disk is unsupported\n",
        "device_map = {\"\": 0}  # or {\"\": torch.cuda.current_device()}\n",
        "base = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map,\n",
        "    trust_remote_code=True,\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "# Optional: SDPA tends to be lighter on memory than eager attention\n",
        "try:\n",
        "    base.config.attn_implementation = \"sdpa\"\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# 2) Attach your LoRA adapters saved earlier (OUTPUT_DIR)\n",
        "infer_model = PeftModel.from_pretrained(base, OUTPUT_DIR)\n",
        "infer_model.eval()\n",
        "try:\n",
        "    infer_model.gradient_checkpointing_disable()\n",
        "except Exception:\n",
        "    pass\n",
        "infer_model.config.use_cache = True\n",
        "\n",
        "# 3) Generation helper: decode only new tokens (donâ€™t echo the prompt)\n",
        "def generate(prompt, max_new_tokens=256, temperature=0.2, top_p=0.9):\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(infer_model.device)\n",
        "    with torch.no_grad():\n",
        "        gen = infer_model.generate(\n",
        "            input_ids=inputs.input_ids,\n",
        "            attention_mask=inputs.attention_mask,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "        )\n",
        "    new_tokens = gen[0, inputs.input_ids.shape[1]:]\n",
        "    return tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "\n",
        "test_prompt = (\n",
        "    \"### Instruction:\\n\"\n",
        "    \"Demonstrate pthread condition variables with a producer/consumer of a single-slot buffer.\\n\\n\"\n",
        "    \"### Response:\\n\"\n",
        ")\n",
        "print(generate(test_prompt, max_new_tokens=1600))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8396b800",
      "metadata": {
        "id": "8396b800"
      },
      "source": [
        "## 9) (Optional) Compile-check the generated C code\n",
        "This cell attempts to extract a C block and compile it with `gcc`. Useful for smoke tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84f35d29",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84f35d29",
        "outputId": "4f3d8859-6806-468a-809f-ccac36ddf626"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Instruction:\n",
            "Provide a minimal C program that prints argc and argv.\n",
            "\n",
            "### Response:\n",
            "\n",
            " Q: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question: Question\n",
            "No ```c fenced block found to compile.\n"
          ]
        }
      ],
      "source": [
        "import re, tempfile, subprocess, textwrap\n",
        "gen = generate('### Instruction:\\nProvide a minimal C program that prints argc and argv.\\n\\n### Response:\\n')\n",
        "print(gen)\n",
        "m = re.search(r'```c\\n(.*?)\\n```', gen, flags=re.S)\n",
        "if m:\n",
        "    code = m.group(1)\n",
        "    with open('/content/test.c','w') as f:\n",
        "        f.write(code)\n",
        "    print('\\n[compile] test.c -> a.out')\n",
        "    proc = subprocess.run(['gcc','/content/test.c','-o','/content/a.out'], capture_output=True, text=True)\n",
        "    print('gcc rc:', proc.returncode)\n",
        "    print('stderr:', proc.stderr[:500])\n",
        "    if proc.returncode==0:\n",
        "        run = subprocess.run(['/content/a.out','hello','world'], capture_output=True, text=True)\n",
        "        print('[run] output:\\n', run.stdout)\n",
        "else:\n",
        "    print('No ```c fenced block found to compile.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a74ef0c",
      "metadata": {
        "id": "4a74ef0c"
      },
      "source": [
        "## 10) (Optional) Merge LoRA into base weights (for export)\n",
        "Merging increases memory needs; consider doing this on a larger GPU. Skippable if you serve with adapters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e4b004e",
      "metadata": {
        "id": "9e4b004e"
      },
      "outputs": [],
      "source": [
        "MERGE = False  # set True to attempt a full merge\n",
        "if MERGE:\n",
        "    from peft import AutoPeftModelForCausalLM\n",
        "    m = AutoPeftModelForCausalLM.from_pretrained(OUTPUT_DIR, device_map='auto')\n",
        "    m = m.merge_and_unload()\n",
        "    m.save_pretrained(OUTPUT_DIR + '-merged', safe_serialization=True)\n",
        "    tokenizer.save_pretrained(OUTPUT_DIR + '-merged')\n",
        "    print('Merged model saved to', OUTPUT_DIR + '-merged')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8975b6e2",
      "metadata": {
        "id": "8975b6e2"
      },
      "source": [
        "## 11) Serving tips\n",
        "- For inference on CPU/GPU with low memory, keep 4-bit base + adapters.\n",
        "- For best latency, run on GPU with `load_in_4bit=True`.\n",
        "- Consider adding a **RAG** layer to inject fresh man pages when required."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06c2fc8b",
      "metadata": {
        "id": "06c2fc8b"
      },
      "source": [
        "## 12) Next steps\n",
        "- Replace `/content/data/sysprog_train.jsonl` with your full dataset.\n",
        "- Increase `EPOCHS`, consider a validation split and early stopping.\n",
        "- Add **safety/eval** harness: unit tests, syscall quizzes, troubleshooting tasks.\n",
        "- Track metrics with Weights & Biases (`wandb`) if desired."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "51dc6f75423f49a3a582e828bafb73c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7ae052dd14db4509bc97c0410811709f",
              "IPY_MODEL_ea9090d7558e464cad1cdecaf5cc924c",
              "IPY_MODEL_2712d28d8a3f46e885f9f241f79cff48"
            ],
            "layout": "IPY_MODEL_8bddd3e8dcf34c3fa44109af0b0e5307"
          }
        },
        "7ae052dd14db4509bc97c0410811709f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85da4ee55d6845fb96e75f60249d8d14",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0ee0f4965f0648ccb22ebe42f2005c7a",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "ea9090d7558e464cad1cdecaf5cc924c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5480ec1764e44750b8f2ed941af983e1",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2c1db38d1dd9456cac708e2221142824",
            "value": 3
          }
        },
        "2712d28d8a3f46e885f9f241f79cff48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_339083708a71403c86992adaf2ea48f7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4bcb5850409343bcb997d62645d9d79d",
            "value": "â€‡3/3â€‡[01:19&lt;00:00,â€‡25.91s/it]"
          }
        },
        "8bddd3e8dcf34c3fa44109af0b0e5307": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85da4ee55d6845fb96e75f60249d8d14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ee0f4965f0648ccb22ebe42f2005c7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5480ec1764e44750b8f2ed941af983e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c1db38d1dd9456cac708e2221142824": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "339083708a71403c86992adaf2ea48f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bcb5850409343bcb997d62645d9d79d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0de12dfea8a443d79ec0b6059e4d9b86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9f954891b9304605a66c75be93709ebc",
              "IPY_MODEL_8991659f0eba4db9b320894c448792d8",
              "IPY_MODEL_41f11733fe38456c9eb2a0800fd54eaa"
            ],
            "layout": "IPY_MODEL_1c6f63c23c09488cad8ac0716126b750"
          }
        },
        "9f954891b9304605a66c75be93709ebc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85f0c9e4e04741709d231ddf2bd1c5a5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_200bc99abf364fef9e74f16f9d39af18",
            "value": "Generatingâ€‡trainâ€‡split:â€‡"
          }
        },
        "8991659f0eba4db9b320894c448792d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5cdc67e161d4739bc7b2797dc147bc9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_427c04eb67ec4c9982034a7b705ff82d",
            "value": 1
          }
        },
        "41f11733fe38456c9eb2a0800fd54eaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a869b29ac5764a618f84e096981beab2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_488eb2d472e04f0496634cc82d405aad",
            "value": "â€‡2/0â€‡[00:00&lt;00:00,â€‡18.23â€‡examples/s]"
          }
        },
        "1c6f63c23c09488cad8ac0716126b750": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85f0c9e4e04741709d231ddf2bd1c5a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "200bc99abf364fef9e74f16f9d39af18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5cdc67e161d4739bc7b2797dc147bc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "427c04eb67ec4c9982034a7b705ff82d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a869b29ac5764a618f84e096981beab2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "488eb2d472e04f0496634cc82d405aad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a2de94f4ab1472d8154b196b1bf574d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0f358e597d784e75aabc83c17121027a",
              "IPY_MODEL_5d8c46d42a174afa82a5328df18300da",
              "IPY_MODEL_77a2651524484c879413d196d8a5b1dc"
            ],
            "layout": "IPY_MODEL_131d78752814405383dfe693b43949ca"
          }
        },
        "0f358e597d784e75aabc83c17121027a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0c39a3dcdb944fdbd8f385fe7c42a0e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f7ffef4bc8364330b9501ef5e36e1088",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "5d8c46d42a174afa82a5328df18300da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_997c61c43a624d9a977692b1b7a2d56b",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_726a55c6e5d847d69eda8a8efc317b6b",
            "value": 3
          }
        },
        "77a2651524484c879413d196d8a5b1dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_067e7895e5934164ba34ec3408acdf61",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f14d96ecefa14c1b8a02021ade99ea3e",
            "value": "â€‡3/3â€‡[01:16&lt;00:00,â€‡25.18s/it]"
          }
        },
        "131d78752814405383dfe693b43949ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0c39a3dcdb944fdbd8f385fe7c42a0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7ffef4bc8364330b9501ef5e36e1088": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "997c61c43a624d9a977692b1b7a2d56b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "726a55c6e5d847d69eda8a8efc317b6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "067e7895e5934164ba34ec3408acdf61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f14d96ecefa14c1b8a02021ade99ea3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}