{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPjpUNktdsQxoG0+FqTjhca"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ab24d4e4663e4b9cb877cf205b729615": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf76989f7b63404ca9b8ed4f096b5063",
              "IPY_MODEL_1c31d15df1414a8cab03c3d33ced6c4e",
              "IPY_MODEL_d32fe7045f90448390a7c51a3fa51a4a"
            ],
            "layout": "IPY_MODEL_cd737679e5344afa88b3e67052488bca"
          }
        },
        "cf76989f7b63404ca9b8ed4f096b5063": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df6540a90e9b43b49eab68af2db229a3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d4102615fe2241f69ca5189ea7885cb7",
            "value": "Map:â€‡100%"
          }
        },
        "1c31d15df1414a8cab03c3d33ced6c4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7546d7bdb4a843399a5fe7ae15133812",
            "max": 4800,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0c3ca9e433e540c9af1325623956b22e",
            "value": 4800
          }
        },
        "d32fe7045f90448390a7c51a3fa51a4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df47752108fe4b3999f902ca3ab50752",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2a21416dc6ce40ca817ae3d747830d5a",
            "value": "â€‡4800/4800â€‡[00:03&lt;00:00,â€‡1643.22â€‡examples/s]"
          }
        },
        "cd737679e5344afa88b3e67052488bca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df6540a90e9b43b49eab68af2db229a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4102615fe2241f69ca5189ea7885cb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7546d7bdb4a843399a5fe7ae15133812": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c3ca9e433e540c9af1325623956b22e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df47752108fe4b3999f902ca3ab50752": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a21416dc6ce40ca817ae3d747830d5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df193915892142fda2e6acb773c50d9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_41397cbb12064f7da5fe0cc7366daf27",
              "IPY_MODEL_ac7da1d366f940f2ad1ec1b14d196f31",
              "IPY_MODEL_60bad51ad9634beb810c91f9547e7d47"
            ],
            "layout": "IPY_MODEL_937d4bb8cce6468eb540d8e2a989032c"
          }
        },
        "41397cbb12064f7da5fe0cc7366daf27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bf7203b503a4b9385acc176c2c3f17b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_44a00bf357bc4422b8e499655665b0a6",
            "value": "Map:â€‡100%"
          }
        },
        "ac7da1d366f940f2ad1ec1b14d196f31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3ebe358c68543c891eecb7fa8fc4925",
            "max": 1200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_12c12af6156245c5a12b293a9e475dde",
            "value": 1200
          }
        },
        "60bad51ad9634beb810c91f9547e7d47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5f8866ad3154ac782c7ebdc3b91e275",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_67f73fb032194657bb35d6a619e98c88",
            "value": "â€‡1200/1200â€‡[00:00&lt;00:00,â€‡2378.25â€‡examples/s]"
          }
        },
        "937d4bb8cce6468eb540d8e2a989032c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bf7203b503a4b9385acc176c2c3f17b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44a00bf357bc4422b8e499655665b0a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3ebe358c68543c891eecb7fa8fc4925": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12c12af6156245c5a12b293a9e475dde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a5f8866ad3154ac782c7ebdc3b91e275": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67f73fb032194657bb35d6a619e98c88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6lFP4iwgRg_1",
        "collapsed": true,
        "outputId": "d1008e94-227a-46c8-8ff5-7aacfa2b5b60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ab24d4e4663e4b9cb877cf205b729615",
            "cf76989f7b63404ca9b8ed4f096b5063",
            "1c31d15df1414a8cab03c3d33ced6c4e",
            "d32fe7045f90448390a7c51a3fa51a4a",
            "cd737679e5344afa88b3e67052488bca",
            "df6540a90e9b43b49eab68af2db229a3",
            "d4102615fe2241f69ca5189ea7885cb7",
            "7546d7bdb4a843399a5fe7ae15133812",
            "0c3ca9e433e540c9af1325623956b22e",
            "df47752108fe4b3999f902ca3ab50752",
            "2a21416dc6ce40ca817ae3d747830d5a",
            "df193915892142fda2e6acb773c50d9f",
            "41397cbb12064f7da5fe0cc7366daf27",
            "ac7da1d366f940f2ad1ec1b14d196f31",
            "60bad51ad9634beb810c91f9547e7d47",
            "937d4bb8cce6468eb540d8e2a989032c",
            "1bf7203b503a4b9385acc176c2c3f17b",
            "44a00bf357bc4422b8e499655665b0a6",
            "d3ebe358c68543c891eecb7fa8fc4925",
            "12c12af6156245c5a12b293a9e475dde",
            "a5f8866ad3154ac782c7ebdc3b91e275",
            "67f73fb032194657bb35d6a619e98c88"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Device: cuda\n",
            "âœ… Dataset: 4800 train | 1200 test | Test buggy=585\n",
            "trainable params: 589,824 || all params: 61,096,448 || trainable%: 0.9654\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/4800 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab24d4e4663e4b9cb877cf205b729615"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4016: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1200 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df193915892142fda2e6acb773c50d9f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1200' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1200/1500 12:14 < 03:03, 1.63 it/s, Epoch 4/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Bertscore F1</th>\n",
              "      <th>Detection Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.563200</td>\n",
              "      <td>1.199999</td>\n",
              "      <td>0.764621</td>\n",
              "      <td>0.691207</td>\n",
              "      <td>0.760538</td>\n",
              "      <td>0.749344</td>\n",
              "      <td>0.827500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.029500</td>\n",
              "      <td>0.990470</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.955700</td>\n",
              "      <td>0.943066</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.937000</td>\n",
              "      <td>0.930508</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [150/150 01:13]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“Š Evaluation: {\n",
            "  \"eval_loss\": 0.9904703497886658,\n",
            "  \"eval_rouge1\": 1.0,\n",
            "  \"eval_rouge2\": 1.0,\n",
            "  \"eval_rougeL\": 1.0,\n",
            "  \"eval_bertscore_f1\": 1.0,\n",
            "  \"eval_detection_accuracy\": 1.0,\n",
            "  \"eval_runtime\": 77.0921,\n",
            "  \"eval_samples_per_second\": 15.566,\n",
            "  \"eval_steps_per_second\": 1.946,\n",
            "  \"epoch\": 4.0\n",
            "}\n",
            "\n",
            "ðŸ”Ž Spot-check predictions vs references\n",
            "\n",
            "[0] PRED: No bug: Simple addition function is correct.\n",
            "    REF:  No bug: Simple addition function is correct.\n",
            "\n",
            "[1] PRED: No bug: String literal used safely with puts.\n",
            "    REF:  No bug: String literal used safely with puts.\n",
            "\n",
            "[2] PRED: Bug: Infinite loop due to wrong update direction. Fix: Use i++ to reach termination.\n",
            "    REF:  Bug: Infinite loop due to wrong update direction. Fix: Use i++ to reach termination.\n",
            "\n",
            "[3] PRED: Bug: gets() is unsafe and can overflow. Fix: Use fgets with size limit.\n",
            "    REF:  Bug: gets() is unsafe and can overflow. Fix: Use fgets with size limit.\n",
            "\n",
            "[4] PRED: Bug: Off-by-one writing arr[10] if arr has size 10. Fix: Use i10.\n",
            "    REF:  Bug: Off-by-one writing arr[10] if arr has size 10. Fix: Use i<10.\n",
            "\n",
            "[5] PRED: No bug: Buffer is large enough for the copied string and null terminator.\n",
            "    REF:  No bug: Buffer is large enough for the copied string and null terminator.\n",
            "\n",
            "[6] PRED: Bug: Buffer overflow (5 bytes into 4). Fix: Use strncpy with size or increase buffer.\n",
            "    REF:  Bug: Buffer overflow (5 bytes into 4). Fix: Use strncpy with size or increase buffer.\n",
            "\n",
            "[7] PRED: Bug: Freeing stack memory. Fix: Only free heap allocations.\n",
            "    REF:  Bug: Freeing stack memory. Fix: Only free heap allocations.\n",
            "\n",
            "[8] PRED: No bug: Buffer is large enough for the copied string and null terminator.\n",
            "    REF:  No bug: Buffer is large enough for the copied string and null terminator.\n",
            "\n",
            "[9] PRED: Bug: Dereferencing uninitialized pointer p. Fix: Allocate or point p to valid memory.\n",
            "    REF:  Bug: Dereferencing uninitialized pointer p. Fix: Allocate or point p to valid memory.\n",
            "\n",
            "ðŸ”¹ Sample test code:\n",
            " You are a C static analysis assistant. Given the code, respond with exactly one sentence that starts with either 'Bug:' or 'No bug:'. If 'Bug:', briefly explain and include a fix. If 'No bug:', briefly justify why it is safe.\n",
            "\n",
            "Find bug in this C code:\n",
            "\n",
            "int sum(int a,int b){return a+b;} int main(){printf(\"%d\",sum(1,2));}\n",
            "\n",
            "ðŸ’¡ Model output:\n",
            " No bug: Simple addition function is correct.\n",
            "ðŸ’¾ Saved LoRA adapters to: ./gen_lora_adapter\n",
            "ðŸ’¾ Saved merged model to: ./gen_merged_model\n",
            "\n",
            "ðŸŒ Launching Gradioâ€¦\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.44.0, however version 4.44.1 is available, please upgrade. \n",
            "--------\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on public URL: https://1a17cdd1f06fde7dbc.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1a17cdd1f06fde7dbc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# ============================================\n",
        "# ðŸš€ Colab-Ready: C Bug Detector + Explainer (T5/FLAN + LoRA)\n",
        "# Instruction-tuned; local metrics; robust decode; anti-echo generation\n",
        "# ============================================\n",
        "\n",
        "# ---------- STEP 0: Install compatible packages ----------\n",
        "!pip install -q \"transformers==4.42.4\" \"accelerate==0.33.0\" \"datasets==2.20.0\" \\\n",
        "               \"peft==0.11.1\" \"gradio==4.44.0\" \\\n",
        "               \"rouge-score==0.1.2\" \"bert-score==0.3.13\"\n",
        "\n",
        "# ---------- STEP 1: Imports, toggles & quiet mode ----------\n",
        "import os, json, random, numpy as np, re, html, shutil, warnings\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    EarlyStoppingCallback,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    set_seed,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Remove rogue directories that can shadow metrics modules\n",
        "for d in (\"/content/rouge\", \"/content/bertscore\"):\n",
        "    if os.path.isdir(d):\n",
        "        shutil.rmtree(d, ignore_errors=True)\n",
        "\n",
        "# Silence deprecation chatter (safe)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", message=\"`torch.cuda.amp.GradScaler\", category=FutureWarning)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); set_seed(SEED)\n",
        "print(\"âœ… Device:\", device)\n",
        "\n",
        "# ---- Model toggle ----\n",
        "# Choose one:\n",
        "USE_FLAN_BASE = False         # set True to use \"google/flan-t5-base\", else T5-small\n",
        "MODEL_NAME = \"google/flan-t5-base\" if USE_FLAN_BASE else \"t5-small\"\n",
        "\n",
        "# LRs suited for chosen base\n",
        "LR = 8e-4 if USE_FLAN_BASE else 1e-3\n",
        "EPOCHS = 8 if USE_FLAN_BASE else 5\n",
        "\n",
        "# ---- Output format instruction (boosts detection accuracy) ----\n",
        "INSTRUCTION = (\n",
        "    \"You are a C static analysis assistant. \"\n",
        "    \"Given the code, respond with exactly one sentence that starts with either \"\n",
        "    \"'Bug:' or 'No bug:'. If 'Bug:', briefly explain and include a fix. \"\n",
        "    \"If 'No bug:', briefly justify why it is safe.\"\n",
        ")\n",
        "\n",
        "# ---------- STEP 2: Build a balanced synthetic dataset ----------\n",
        "bug_templates = [\n",
        "    (\"int main(){int arr[3]; arr[3] = 7; return 0;}\",\n",
        "     \"Bug: Out-of-bounds array access at arr[3]. Fix: Valid indices are 0â€“2.\"),\n",
        "    (\"char *s=NULL; *s='a';\",\n",
        "     \"Bug: Dereferencing NULL pointer. Fix: Allocate memory before dereference.\"),\n",
        "    (\"int *p=malloc(sizeof(int)); if(p) return 0;\",\n",
        "     \"Bug: Memory leak. Fix: Call free(p) on all successful allocation paths.\"),\n",
        "    (\"int x; if(x==1){printf(\\\"ok\\\");}\",\n",
        "     \"Bug: Use of uninitialized variable x. Fix: Initialize x before use.\"),\n",
        "    (\"char buf[4]; strcpy(buf,\\\"test\\\");\",\n",
        "     \"Bug: Buffer overflow (5 bytes into 4). Fix: Use strncpy with size or increase buffer.\"),\n",
        "    (\"int *p=malloc(sizeof(int)); free(p); free(p);\",\n",
        "      \"Bug: Double free. Fix: Avoid freeing the same pointer twice; set to NULL after free.\"),\n",
        "    (\"char buf[8]; gets(buf);\",\n",
        "     \"Bug: gets() is unsafe and can overflow. Fix: Use fgets with size limit.\"),\n",
        "    (\"int main(){for(int i=0;i<10;i--){printf(\\\"%d\\\",i);} }\",\n",
        "     \"Bug: Infinite loop due to wrong update direction. Fix: Use i++ to reach termination.\"),\n",
        "    (\"FILE *f=fopen(\\\"file.txt\\\",\\\"r\\\"); fclose(f); fclose(f);\",\n",
        "     \"Bug: Double fclose. Fix: Ensure fclose is called once per opened file.\"),\n",
        "    (\"int a=2147483647; int b=a+1;\",\n",
        "     \"Bug: Signed integer overflow when adding 1 to INT_MAX. Fix: Use wider type or check bounds.\"),\n",
        "    (\"char *s; strcpy(s, \\\"hello\\\");\",\n",
        "     \"Bug: Using strcpy on uninitialized pointer 's'. Fix: Allocate memory before use.\"),\n",
        "    (\"int *p; *p = 5;\",\n",
        "     \"Bug: Dereferencing uninitialized pointer p. Fix: Allocate or point p to valid memory.\"),\n",
        "    (\"int a=10; int *p=&a; free(p);\",\n",
        "     \"Bug: Freeing stack memory. Fix: Only free heap allocations.\"),\n",
        "    (\"int i; for(i=0;i<=10;i++){ arr[i]=i; }\",\n",
        "     \"Bug: Off-by-one writing arr[10] if arr has size 10. Fix: Use i<10.\"),\n",
        "    (\"pthread_mutex_t m; int x=0; // write x in threads without locking\",\n",
        "     \"Bug: Potential data race on shared variable x. Fix: Protect accesses with mutex lock/unlock.\"),\n",
        "]\n",
        "\n",
        "clean_templates = [\n",
        "    (\"int main(){int x=0; printf(\\\"%d\\\", x); return 0;}\",\n",
        "     \"No bug: Code initializes and prints x correctly.\"),\n",
        "    (\"char s[6]; strcpy(s, \\\"hi\\\");\",\n",
        "     \"No bug: Buffer is large enough for the copied string and null terminator.\"),\n",
        "    (\"int arr[3]={1,2,3}; for(int i=0;i<3;i++){ printf(\\\"%d\\\", arr[i]); }\",\n",
        "     \"No bug: Proper array bounds in loop.\"),\n",
        "    (\"FILE *f=fopen(\\\"file.txt\\\",\\\"w\\\"); if(f){ fprintf(f,\\\"ok\\\"); fclose(f);} return 0;\",\n",
        "     \"No bug: File opened, used, and closed safely.\"),\n",
        "    (\"int *p=(int*)malloc(sizeof(int)); if(p){ *p=5; free(p);} return 0;\",\n",
        "     \"No bug: Heap memory allocated, used, and freed correctly.\"),\n",
        "    (\"char buf[8]; snprintf(buf, sizeof(buf), \\\"%s\\\", \\\"ok\\\");\",\n",
        "     \"No bug: snprintf prevents overflow by size limiting.\"),\n",
        "    (\"struct S{int a;}; struct S s={.a=1}; printf(\\\"%d\\\", s.a);\",\n",
        "     \"No bug: Struct is initialized before use.\"),\n",
        "    (\"int sum(int a,int b){return a+b;} int main(){printf(\\\"%d\\\",sum(1,2));}\",\n",
        "     \"No bug: Simple addition function is correct.\"),\n",
        "    (\"for(int i=0;i<10;i++){ /* work */ }\",\n",
        "     \"No bug: Loop bounds and update are correct.\"),\n",
        "    (\"const char* msg = \\\"hello\\\"; puts(msg);\",\n",
        "     \"No bug: String literal used safely with puts.\"),\n",
        "]\n",
        "\n",
        "TOTAL_EXAMPLES = 6000  # can increase to 1000+ for better results if GPU time allows\n",
        "half = TOTAL_EXAMPLES // 2\n",
        "examples = []\n",
        "PREFIX = \"Find bug in this C code:\"\n",
        "\n",
        "for _ in range(half):\n",
        "    code, expl = random.choice(bug_templates)\n",
        "    resp = expl if expl.lower().startswith(\"bug:\") else f\"Bug: {expl}\"\n",
        "    prompt = f\"{INSTRUCTION}\\n\\n{PREFIX}\\n\\n{code}\"\n",
        "    examples.append({\"prompt\": prompt, \"response\": resp})\n",
        "\n",
        "for _ in range(half):\n",
        "    code, expl = random.choice(clean_templates)\n",
        "    resp = expl if expl.lower().startswith(\"no bug\") else f\"No bug: {expl}\"\n",
        "    prompt = f\"{INSTRUCTION}\\n\\n{PREFIX}\\n\\n{code}\"\n",
        "    examples.append({\"prompt\": prompt, \"response\": resp})\n",
        "\n",
        "random.shuffle(examples)\n",
        "train_size = int(0.8 * len(examples))\n",
        "train_data, test_data = examples[:train_size], examples[train_size:]\n",
        "\n",
        "os.makedirs(\"c_bug_dataset\", exist_ok=True)\n",
        "with open(\"c_bug_dataset/c_bugs_train.json\", \"w\") as f: json.dump(train_data, f, indent=2)\n",
        "with open(\"c_bug_dataset/c_bugs_test.json\", \"w\") as f: json.dump(test_data, f, indent=2)\n",
        "\n",
        "print(f\"âœ… Dataset: {len(train_data)} train | {len(test_data)} test \"\n",
        "      f\"| Test buggy={sum(d['response'].lower().startswith('bug:') for d in test_data)}\")\n",
        "\n",
        "# ---------- STEP 3: Model + LoRA ----------\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Optional: reduce memory & improve stability\n",
        "try:\n",
        "    base_model.gradient_checkpointing_enable()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "lora_cfg = LoraConfig(\n",
        "    r=8, lora_alpha=16, lora_dropout=0.1, bias=\"none\",\n",
        "    target_modules=[\"q\", \"k\", \"v\", \"o\"], task_type=\"SEQ_2_SEQ_LM\"\n",
        ")\n",
        "model = get_peft_model(base_model, lora_cfg)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# ---------- STEP 4: Preprocessing ----------\n",
        "INPUT_MAX_LEN = 256\n",
        "OUTPUT_MAX_LEN = 128\n",
        "\n",
        "def preprocess_batch(batch: Dict[str, List[str]]) -> Dict[str, Any]:\n",
        "    model_inputs = tokenizer(\n",
        "        batch[\"prompt\"], truncation=True, padding=\"max_length\", max_length=INPUT_MAX_LEN\n",
        "    )\n",
        "    # Tokenize targets; mask pad with -100\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            batch[\"response\"], truncation=True, padding=\"max_length\", max_length=OUTPUT_MAX_LEN\n",
        "        )\n",
        "    labels_ids = [\n",
        "        [(tid if tid != tokenizer.pad_token_id else -100) for tid in seq]\n",
        "        for seq in labels[\"input_ids\"]\n",
        "    ]\n",
        "    model_inputs[\"labels\"] = labels_ids\n",
        "    return model_inputs\n",
        "\n",
        "train_ds = Dataset.from_list(train_data).map(preprocess_batch, batched=True, remove_columns=[\"prompt\", \"response\"])\n",
        "test_ds  = Dataset.from_list(test_data ).map(preprocess_batch, batched=True, remove_columns=[\"prompt\", \"response\"])\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "# ---------- STEP 5: Metrics (Local-only; no evaluate.load) ----------\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score as bert_score_fn\n",
        "\n",
        "def _normalize(xs):\n",
        "    return [re.sub(r\"\\s+\", \" \", x).strip() for x in xs]\n",
        "\n",
        "class LocalRouge:\n",
        "    def compute(self, predictions, references, use_stemmer=True):\n",
        "        scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=use_stemmer)\n",
        "        n = len(predictions)\n",
        "        sums = {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0}\n",
        "        for p, r in zip(predictions, references):\n",
        "            s = scorer.score(r, p)  # (target, prediction)\n",
        "            sums[\"rouge1\"] += s[\"rouge1\"].fmeasure\n",
        "            sums[\"rouge2\"] += s[\"rouge2\"].fmeasure\n",
        "            sums[\"rougeL\"] += s[\"rougeL\"].fmeasure\n",
        "        return {k: v / max(n, 1) for k, v in sums.items()}\n",
        "\n",
        "class LocalBERTScore:\n",
        "    # Use a lighter backbone to avoid Roberta pooler warnings\n",
        "    def __init__(self, model_type=\"microsoft/deberta-base-mnli\"):\n",
        "        self.model_type = model_type\n",
        "    def compute(self, predictions, references, lang=\"en\"):\n",
        "        P, R, F1 = bert_score_fn(\n",
        "            predictions, references, lang=lang,\n",
        "            model_type=self.model_type, rescale_with_baseline=True\n",
        "        )\n",
        "        return {\"precision\": float(P.mean()), \"recall\": float(R.mean()), \"f1\": float(F1.mean())}\n",
        "\n",
        "rouge = LocalRouge()\n",
        "bertscore = LocalBERTScore(model_type=\"microsoft/deberta-base-mnli\")\n",
        "\n",
        "def _to_int_ids(arr, pad_id: int):\n",
        "    \"\"\"Map negatives to pad_id; ensure int dtype; return as list[list[int]].\"\"\"\n",
        "    arr = np.asarray(arr[0] if isinstance(arr, tuple) else arr)\n",
        "    if arr.dtype.kind not in \"iu\":\n",
        "        arr = arr.astype(np.int64)\n",
        "    arr = np.where(arr < 0, pad_id, arr)\n",
        "    if arr.ndim == 1:\n",
        "        arr = arr[None, :]\n",
        "    return [[int(x) for x in row.tolist()] for row in arr]\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    preds, labels = eval_pred\n",
        "    pred_ids = _to_int_ids(preds, pad_id=tokenizer.pad_token_id)\n",
        "    label_ids = _to_int_ids(labels, pad_id=tokenizer.pad_token_id)\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "    decoded_preds = _normalize(decoded_preds)\n",
        "    decoded_labels = _normalize(decoded_labels)\n",
        "\n",
        "    r = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    metrics = dict(r)\n",
        "    bs = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
        "    metrics[\"bertscore_f1\"] = float(bs[\"f1\"])\n",
        "\n",
        "    # Derived detection accuracy\n",
        "    def detect_flag(text: str) -> int:\n",
        "        t = text.lower()\n",
        "        if t.startswith(\"no bug\") or \"no bug\" in t: return 0\n",
        "        if \"bug:\" in t: return 1\n",
        "        return 1 if (\"fix\" in t and \"no bug\" not in t) else 0\n",
        "\n",
        "    y_pred = [detect_flag(p) for p in decoded_preds]\n",
        "    y_true = [1 if lbl.lower().startswith(\"bug:\") else 0 for lbl in decoded_labels]\n",
        "    metrics[\"detection_accuracy\"] = float((np.array(y_pred) == np.array(y_true)).mean().item())\n",
        "    return metrics\n",
        "\n",
        "# ---------- STEP 6: Training config (Seq2SeqTrainer) ----------\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./gen_results\",\n",
        "    evaluation_strategy=\"epoch\",     # deprecation warnings suppressed above\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=LR,                 # auto-adjusted by toggle\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=2,    # effective batch size 16\n",
        "    num_train_epochs=EPOCHS,          # longer for FLAN base\n",
        "    weight_decay=0.01,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=OUTPUT_MAX_LEN,\n",
        "    logging_dir=\"./gen_logs\",\n",
        "    logging_steps=20,\n",
        "    label_smoothing_factor=0.05,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"rougeL\",\n",
        "    greater_is_better=True,\n",
        "    seed=SEED,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=test_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
        ")\n",
        "\n",
        "# ---------- STEP 7: Train & evaluate (safe fallback) ----------\n",
        "def _norm(xs): return [re.sub(r\"\\s+\", \" \", x).strip() for x in xs]\n",
        "\n",
        "def generate_explanation(code_or_prompt: str,\n",
        "                         num_beams: int = 6,\n",
        "                         max_len: int = 128,\n",
        "                         no_repeat_ngram_size: int = 3,\n",
        "                         repetition_penalty: float = 1.15,\n",
        "                         length_penalty: float = 0.9,\n",
        "                         min_new_tokens: int = 8,\n",
        "                         force_prefix: bool = False) -> str:\n",
        "    \"\"\"\n",
        "    Prefix-safe, HTML-unescaped, anti-echo generation.\n",
        "    Set force_prefix=True to experiment with hard prefixing 'Bug:' or 'No bug:' (optional).\n",
        "    \"\"\"\n",
        "    text = html.unescape(code_or_prompt or \"\").strip()\n",
        "    if text.lower().startswith(PREFIX.lower()):\n",
        "        prompt = f\"{INSTRUCTION}\\n\\n{text}\"\n",
        "    else:\n",
        "        prompt = f\"{INSTRUCTION}\\n\\n{PREFIX}\\n\\n{text}\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=INPUT_MAX_LEN)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    gen_kwargs = dict(\n",
        "        max_length=max_len,\n",
        "        num_beams=num_beams,\n",
        "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        length_penalty=length_penalty,\n",
        "        min_new_tokens=min_new_tokens,\n",
        "        early_stopping=True,\n",
        "    )\n",
        "    # Optional: experiment with forced prefix (commented out by default)\n",
        "    # if force_prefix:\n",
        "    #     bug_ids = tokenizer(\"Bug:\", return_tensors=\"pt\")[\"input_ids\"][0].to(model.device)\n",
        "    #     gen_kwargs[\"prefix_allowed_tokens_fn\"] = lambda batch_id, sent: bug_ids.tolist()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, **gen_kwargs)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "def manual_eval():\n",
        "    preds, refs = [], []\n",
        "    for ex in test_data:\n",
        "        out = generate_explanation(ex[\"prompt\"], num_beams=6, max_len=OUTPUT_MAX_LEN)\n",
        "        preds.append(out); refs.append(ex[\"response\"])\n",
        "    preds_n, refs_n = _norm(preds), _norm(refs)\n",
        "    r = rouge.compute(predictions=preds_n, references=refs_n)\n",
        "    metrics = dict(r)\n",
        "    bs = bertscore.compute(predictions=preds_n, references=refs_n, lang=\"en\")\n",
        "    metrics[\"bertscore_f1\"] = float(bs[\"f1\"])\n",
        "    def detect_flag(t: str) -> int:\n",
        "        t = t.lower()\n",
        "        if t.startswith(\"no bug\") or \"no bug\" in t: return 0\n",
        "        if \"bug:\" in t: return 1\n",
        "        return 1 if (\"fix\" in t and \"no bug\" not in t) else 0\n",
        "    y_pred = [detect_flag(p) for p in preds_n]\n",
        "    y_true = [1 if r.lower().startswith(\"bug:\") else 0 for r in refs_n]\n",
        "    metrics[\"detection_accuracy\"] = float((np.array(y_pred) == np.array(y_true)).mean().item())\n",
        "    return metrics\n",
        "\n",
        "try:\n",
        "    trainer.train()\n",
        "    eval_res = trainer.evaluate()\n",
        "    print(\"ðŸ“Š Evaluation:\", json.dumps(eval_res, indent=2))\n",
        "except Exception as e:\n",
        "    print(\"âš ï¸ Training-time evaluation failed; doing manual post-training eval.\\n\", repr(e))\n",
        "    eval_res = manual_eval()\n",
        "    print(\"ðŸ“Š Manual Evaluation:\", json.dumps(eval_res, indent=2))\n",
        "\n",
        "# ---------- STEP 8: Spot-check 10 samples ----------\n",
        "print(\"\\nðŸ”Ž Spot-check predictions vs references\")\n",
        "for i, ex in enumerate(test_data[:10]):\n",
        "    pred = generate_explanation(ex[\"prompt\"], num_beams=6, max_len=OUTPUT_MAX_LEN)\n",
        "    print(f\"\\n[{i}] PRED: {pred}\\n    REF:  {ex['response']}\")\n",
        "\n",
        "# ---------- STEP 9: Sanity sample (code-only to avoid double prefix) ----------\n",
        "sample_prompt = test_data[0][\"prompt\"]\n",
        "sample_code_only = re.sub(r\"(?i)^.*find bug in this c code:\\s*\", \"\", sample_prompt).strip()\n",
        "print(\"\\nðŸ”¹ Sample test code:\\n\", sample_code_only)\n",
        "print(\"\\nðŸ’¡ Model output:\\n\", generate_explanation(sample_code_only))\n",
        "\n",
        "# ---------- STEP 10: Save artifacts (LoRA adapters + optional merged model) ----------\n",
        "ADAPTER_DIR = \"./gen_lora_adapter\"\n",
        "os.makedirs(ADAPTER_DIR, exist_ok=True)\n",
        "model.save_pretrained(ADAPTER_DIR)\n",
        "tokenizer.save_pretrained(ADAPTER_DIR)\n",
        "print(f\"ðŸ’¾ Saved LoRA adapters to: {ADAPTER_DIR}\")\n",
        "\n",
        "MERGED_DIR = \"./gen_merged_model\"\n",
        "try:\n",
        "    merged = model.merge_and_unload()\n",
        "    merged.save_pretrained(MERGED_DIR)\n",
        "    tokenizer.save_pretrained(MERGED_DIR)\n",
        "    print(f\"ðŸ’¾ Saved merged model to: {MERGED_DIR}\")\n",
        "except Exception as e:\n",
        "    print(\"âš ï¸ Could not merge LoRA into base model; continue with adapters.\\n\", e)\n",
        "\n",
        "# ---------- STEP 11: Gradio UI ----------\n",
        "import gradio as gr\n",
        "def ui_predict(text):\n",
        "    return generate_explanation(text, num_beams=6, max_len=OUTPUT_MAX_LEN)\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=ui_predict,\n",
        "    inputs=gr.Textbox(lines=12, placeholder=\"Paste C code OR a full prompt like 'Find bug in this C code: ...'\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"C Bug Detector + Explainer (T5/FLAN + LoRA)\",\n",
        "    description=\"Detects if there is a bug and explains the fix. Instruction-tuned; accepts code or full prompt.\",\n",
        ")\n",
        "print(\"\\nðŸŒ Launching Gradioâ€¦\")\n",
        "demo.launch(share=True)\n"
      ]
    }
  ]
}