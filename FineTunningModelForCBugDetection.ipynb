{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2eNlW0dU2cfi52C9fR4Q/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lFP4iwgRg_1"
      },
      "outputs": [],
      "source": [
        "# =============================\n",
        "# âœ… STEP 0: Install Dependencies\n",
        "# =============================\n",
        "!pip install -q transformers datasets peft accelerate evaluate gradio bert-score\n",
        "\n",
        "# =============================\n",
        "# âœ… STEP 1: Imports & Setup\n",
        "# =============================\n",
        "import os, json, random, numpy as np, re\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    set_seed,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import evaluate\n",
        "import gradio as gr\n",
        "\n",
        "# Check GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"âœ… Using device: {device}\")\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "set_seed(SEED)\n",
        "\n",
        "# =============================\n",
        "# âœ… STEP 2: Build Dataset\n",
        "# =============================\n",
        "bug_templates = [\n",
        "    (\"int main(){int arr[3]; arr[3] = 7; return 0;}\",\n",
        "     \"Bug: Out-of-bounds array access at arr[3]. Fix: Valid indices are 0â€“2.\"),\n",
        "    (\"char *s=NULL; *s='a';\",\n",
        "     \"Bug: Dereferencing NULL pointer. Fix: Allocate memory before dereference.\"),\n",
        "    (\"int *p=malloc(sizeof(int)); if(p) return 0;\",\n",
        "     \"Bug: Memory leak. Fix: Call free(p) on all successful allocation paths.\"),\n",
        "    (\"int x; if(x==1){printf(\\\"ok\\\");}\",\n",
        "     \"Bug: Use of uninitialized variable x. Fix: Initialize x before use.\"),\n",
        "    (\"char buf[4]; strcpy(buf,\\\"test\\\");\",\n",
        "     \"Bug: Buffer overflow (5 bytes into 4). Fix: Use strncpy with size or increase buffer.\"),\n",
        "    (\"int *p=malloc(sizeof(int)); free(p); free(p);\",\n",
        "     \"Bug: Double free. Fix: Avoid freeing the same pointer twice; set to NULL after free.\"),\n",
        "    (\"char buf[8]; gets(buf);\",\n",
        "     \"Bug: gets() is unsafe and can overflow. Fix: Use fgets with size limit.\"),\n",
        "    (\"int main(){for(int i=0;i<10;i--){printf(\\\"%d\\\",i);} }\",\n",
        "     \"Bug: Infinite loop due to wrong update direction. Fix: Use i++ to reach termination.\"),\n",
        "    (\"FILE *f=fopen(\\\"file.txt\\\",\\\"r\\\"); fclose(f); fclose(f);\",\n",
        "     \"Bug: Double fclose. Fix: Ensure fclose is called once per opened file.\"),\n",
        "    (\"int a=2147483647; int b=a+1;\",\n",
        "     \"Bug: Signed integer overflow when adding 1 to INT_MAX. Fix: Use wider type or check bounds.\"),\n",
        "]\n",
        "\n",
        "clean_templates = [\n",
        "    (\"int main(){int x=0; printf(\\\"%d\\\", x); return 0;}\",\n",
        "     \"No bug: Code initializes and prints x correctly.\"),\n",
        "    (\"char s[6]; strcpy(s, \\\"hi\\\");\",\n",
        "     \"No bug: Buffer is large enough for the copied string and null terminator.\"),\n",
        "    (\"int arr[3]={1,2,3}; for(int i=0;i<3;i++){ printf(\\\"%d\\\", arr[i]); }\",\n",
        "     \"No bug: Proper array bounds in loop.\"),\n",
        "    (\"FILE *f=fopen(\\\"file.txt\\\",\\\"w\\\"); if(f){ fprintf(f,\\\"ok\\\"); fclose(f);} return 0;\",\n",
        "     \"No bug: File opened, used, and closed safely.\"),\n",
        "]\n",
        "\n",
        "examples = []\n",
        "TOTAL_EXAMPLES = 200\n",
        "half = TOTAL_EXAMPLES // 2\n",
        "\n",
        "for _ in range(half):\n",
        "    code, bugfix = random.choice(bug_templates)\n",
        "    examples.append({\"prompt\": f\"Find bug in this C code:\\n\\n{code}\", \"response\": bugfix})\n",
        "\n",
        "for _ in range(half):\n",
        "    code, expl = random.choice(clean_templates)\n",
        "    examples.append({\"prompt\": f\"Find bug in this C code:\\n\\n{code}\", \"response\": expl})\n",
        "\n",
        "random.shuffle(examples)\n",
        "train_size = int(0.8 * len(examples))\n",
        "train_data, test_data = examples[:train_size], examples[train_size:]\n",
        "\n",
        "print(f\"âœ… Dataset ready: {len(train_data)} train, {len(test_data)} test\")\n",
        "\n",
        "# =============================\n",
        "# âœ… STEP 3: Model + LoRA Setup\n",
        "# =============================\n",
        "model_name = \"t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"q\", \"k\", \"v\", \"o\"],\n",
        "    task_type=\"SEQ_2_SEQ_LM\",\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# =============================\n",
        "# âœ… STEP 4: Preprocessing\n",
        "# =============================\n",
        "INPUT_MAX_LEN = 256\n",
        "OUTPUT_MAX_LEN = 128\n",
        "\n",
        "def preprocess_batch(batch):\n",
        "    model_inputs = tokenizer(batch[\"prompt\"], truncation=True, padding=\"max_length\", max_length=INPUT_MAX_LEN)\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(batch[\"response\"], truncation=True, padding=\"max_length\", max_length=OUTPUT_MAX_LEN)\n",
        "    labels_ids = [[(tid if tid != tokenizer.pad_token_id else -100) for tid in seq] for seq in labels[\"input_ids\"]]\n",
        "    model_inputs[\"labels\"] = labels_ids\n",
        "    return model_inputs\n",
        "\n",
        "train_dataset = Dataset.from_list(train_data).map(preprocess_batch, batched=True, remove_columns=[\"prompt\", \"response\"])\n",
        "test_dataset = Dataset.from_list(test_data).map(preprocess_batch, batched=True, remove_columns=[\"prompt\", \"response\"])\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "# =============================\n",
        "# âœ… STEP 5: Training Config\n",
        "# =============================\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gen_results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-4,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=OUTPUT_MAX_LEN,\n",
        "    logging_dir=\"./gen_logs\",\n",
        "    logging_steps=20,\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"rougeL\",\n",
        "    greater_is_better=True,\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "# =============================\n",
        "# âœ… STEP 6: Metrics\n",
        "# =============================\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bertscore = evaluate.load(\"bertscore\")\n",
        "\n",
        "def normalize_text_list(texts):\n",
        "    return [re.sub(r\"\\s+\", \" \", t).strip() for t in texts]\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    preds, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    decoded_preds = normalize_text_list(decoded_preds)\n",
        "    decoded_labels = normalize_text_list(decoded_labels)\n",
        "    rouge_res = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    bs = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
        "    metrics = dict(rouge_res)\n",
        "    metrics[\"bertscore_f1\"] = float(np.mean(bs[\"f1\"]))\n",
        "    return metrics\n",
        "\n",
        "# =============================\n",
        "# âœ… STEP 7: Train\n",
        "# =============================\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"ðŸ“Š Evaluation Results:\", json.dumps(eval_results, indent=2))\n",
        "\n",
        "# =============================\n",
        "# âœ… STEP 8: Inference + Gradio UI\n",
        "# =============================\n",
        "def generate_explanation(code_snippet):\n",
        "    prompt = f\"Find bug in this C code:\\n\\n{code_snippet}\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=INPUT_MAX_LEN).to(device)\n",
        "    outputs = model.generate(**inputs, max_length=OUTPUT_MAX_LEN, num_beams=4)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Quick test\n",
        "print(\"\\nðŸ”¹ Sample Test:\\n\", test_data[0][\"prompt\"])\n",
        "print(\"\\nðŸ’¡ Prediction:\\n\", generate_explanation(test_data[0][\"prompt\"]))\n",
        "\n",
        "# Launch Gradio UI\n",
        "gr.Interface(\n",
        "    fn=generate_explanation,\n",
        "    inputs=gr.Textbox(lines=12, placeholder=\"Paste C code here...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"C Bug Detector + Explainer (T5 + LoRA)\",\n",
        "    description=\"Detects if there is a bug and explains the fix.\"\n",
        ").launch(share=True)\n"
      ]
    }
  ]
}