{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPV7il3dw1vgS+IqntFFdT2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "id": "xgpMHwal4-5H",
        "outputId": "7db395ef-af2b-41d9-84db-5c30bd58de14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Found existing installation: pandas 2.2.2\n",
            "Uninstalling pandas-2.2.2:\n",
            "  Successfully uninstalled pandas-2.2.2\n",
            "Found existing installation: datasets 2.20.0\n",
            "Uninstalling datasets-2.20.0:\n",
            "  Successfully uninstalled datasets-2.20.0\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "yfinance 0.2.65 requires websockets>=13.0, but you have websockets 12.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1521262866.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m from transformers import (\n\u001b[1;32m     19\u001b[0m     \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2.20.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReadInstruction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBeamBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBuilderConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorBasedBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfsspec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) from _err\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from pandas._config import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mget_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mset_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"warn_copy_on_write\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdates\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from pandas._config.config import (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# üöÄ Colab-Ready: C Bug Detector + Explainer (T5/FLAN + LoRA)\n",
        "# Synthetic dataset in requested schema; local metrics; robust decode; anti-echo generation\n",
        "# ============================================\n",
        "\n",
        "# ---------- STEP 0: Install compatible packages ----------\n",
        "# Uninstall potentially conflicting versions first\n",
        "!pip uninstall -y numpy pandas datasets\n",
        "# Install specific versions for compatibility\n",
        "!pip install -q \"transformers==4.42.4\" \"accelerate==0.33.0\" \"datasets==2.20.0\" \\\n",
        "               \"peft==0.11.1\" \"gradio==4.44.0\" \\\n",
        "               \"rouge-score==0.1.2\" \"bert-score==0.3.13\" \\\n",
        "               \"numpy==1.26.4\" \"pandas==2.2.2\" # Add specific versions for compatibility\n",
        "\n",
        "# ---------- STEP 1: Imports, toggles & quiet mode ----------\n",
        "import os, json, random, numpy as np, re, html, shutil, warnings, uuid\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    EarlyStoppingCallback,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    set_seed,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Remove rogue directories that can shadow modules\n",
        "for d in (\"/content/rouge\", \"/content/bertscore\"):\n",
        "    if os.path.isdir(d):\n",
        "        shutil.rmtree(d, ignore_errors=True)\n",
        "\n",
        "# Silence deprecation chatter\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", message=\"`torch.cuda.amp.GradScaler\", category=FutureWarning)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); set_seed(SEED)\n",
        "print(\"‚úÖ Device:\", device)\n",
        "\n",
        "# ---- Model toggle ----\n",
        "# Choose one:\n",
        "USE_FLAN_BASE = False             # set True to use \"google/flan-t5-base\", else \"t5-small\"\n",
        "MODEL_NAME = \"google/flan-t5-base\" if USE_FLAN_BASE else \"t5-small\"\n",
        "LR = 8e-4 if USE_FLAN_BASE else 1e-3\n",
        "EPOCHS = 8 if USE_FLAN_BASE else 5\n",
        "\n",
        "# ---- Task constants ----\n",
        "PREFIX = \"Find bug in this C code:\"\n",
        "SOURCE = \"synthetic\"\n",
        "\n",
        "# ============================================\n",
        "# STEP 2: Synthetic dataset generation (requested schema)\n",
        "# ============================================\n",
        "\n",
        "def esc(s: str) -> str:\n",
        "    \"\"\"Escape < and > for inclusion in JSON text strings (to match your example).\"\"\"\n",
        "    return html.escape(s)\n",
        "\n",
        "def make_bug_item(code: str, fixed_code: str, bug_type: str, difficulty: str = \"easy\") -> Dict[str, Any]:\n",
        "    \"\"\"Create one buggy example (label=1) in the requested schema.\"\"\"\n",
        "    pid = str(uuid.uuid4())\n",
        "    prompt = f\"{PREFIX}\\n\\n{esc(code)}\"\n",
        "    response = (\n",
        "        f\"Bug: {bug_type}. Provide a safe fix. Example fix:\\n{esc(fixed_code)}\"\n",
        "    )\n",
        "    return {\n",
        "        \"id\": pid,\n",
        "        \"prompt\": prompt,\n",
        "        \"response\": response,\n",
        "        \"label\": 1,\n",
        "        \"bug_type\": bug_type,\n",
        "        \"code\": esc(code),\n",
        "        \"fixed_code\": esc(fixed_code),\n",
        "        \"difficulty\": difficulty,\n",
        "        \"source\": SOURCE,\n",
        "    }\n",
        "\n",
        "def make_clean_item(code: str, justification: str = \"Code is safe; no changes needed.\", difficulty: str = \"easy\") -> Dict[str, Any]:\n",
        "    \"\"\"Create one clean example (label=0) in the requested schema.\"\"\"\n",
        "    pid = str(uuid.uuid4())\n",
        "    prompt = f\"{PREFIX}\\n\\n{esc(code)}\"\n",
        "    response = f\"No bug: {justification}\"\n",
        "    return {\n",
        "        \"id\": pid,\n",
        "        \"prompt\": prompt,\n",
        "        \"response\": response,\n",
        "        \"label\": 0,\n",
        "        \"bug_type\": \"No bug\",\n",
        "        \"code\": esc(code),\n",
        "        \"fixed_code\": esc(code),   # same as code for clean examples\n",
        "        \"difficulty\": difficulty,\n",
        "        \"source\": SOURCE,\n",
        "    }\n",
        "\n",
        "# ---- Buggy templates with explicit fixes ----\n",
        "bug_cases: List[Tuple[str, str, str, str]] = [\n",
        "    # (buggy_code, fixed_code, bug_type, difficulty)\n",
        "    (\n",
        "        # Buffer overflow (strcpy)\n",
        "        \"\"\"#include <stdio.h>\n",
        "#include <string.h>\n",
        "\n",
        "int main(){\n",
        "    char c6[3];\n",
        "    strcpy(c6, \"hix\");\n",
        "    printf(\"%s\\\\n\", c6);\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"\"\"#include <stdio.h>\n",
        "#include <string.h>\n",
        "\n",
        "int main(){\n",
        "    char c6[4];\n",
        "    strncpy(c6, \"hix\", sizeof(c6)-1);\n",
        "    c6[sizeof(c6)-1] = '\\\\0';\n",
        "    printf(\"%s\\\\n\", c6);\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"Buffer overflow (strcpy)\",\n",
        "        \"easy\",\n",
        "    ),\n",
        "    (\n",
        "        # Out of bounds write\n",
        "        \"\"\"#include <stdio.\n",
        "int main(){\n",
        "    int arr[3];\n",
        "    arr[3] = 7;\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"\"\"#include <stdio.h>\n",
        "int main(){\n",
        "    int arr[3];\n",
        "    arr[2] = 7; // indices 0..2 valid\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"Out-of-bounds array access\",\n",
        "        \"easy\",\n",
        "    ),\n",
        "    (\n",
        "        # NULL deref\n",
        "        \"\"\"#include <stdlib.h>\n",
        "int main(){\n",
        "    char *s = NULL;\n",
        "    *s = 'a';\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"\"\"#include <stdlib.h>\n",
        "int main(){\n",
        "    char *s = (char*)malloc(2);\n",
        "    if(!s) return 1;\n",
        "    s[0] = 'a';\n",
        "    s[1] = '\\\\0';\n",
        "    free(s);\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"Dereferencing NULL pointer\",\n",
        "        \"easy\",\n",
        "    ),\n",
        "    (\n",
        "        # Memory leak due to early return\n",
        "        \"\"\"#include <stdlib.h>\n",
        "int main(){\n",
        "    int *p = (int*)malloc(sizeof(int));\n",
        "    if(p) return 0; // leaked\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"\"\"#include <stdlib.h>\n",
        "int main(){\n",
        "    int *p = (int*)malloc(sizeof(int));\n",
        "    if(p){\n",
        "        // ... use p ...\n",
        "        free(p); // free before returning\n",
        "        return 0;\n",
        "    }\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"Memory leak (missing free on success path)\",\n",
        "        \"easy\",\n",
        "    ),\n",
        "    (\n",
        "        # Uninitialized variable\n",
        "        \"\"\"#include <stdio.h>\n",
        "int main(){\n",
        "    int x;\n",
        "    if(x == 1){\n",
        "        printf(\"ok\");\n",
        "    }\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"\"\"#include <stdio.h>\n",
        "int main(){\n",
        "    int x = 0; // initialize\n",
        "    if(x == 1){\n",
        "        printf(\"ok\");\n",
        "    }\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"Use of uninitialized variable\",\n",
        "        \"easy\",\n",
        "    ),\n",
        "    (\n",
        "        # Double free\n",
        "        \"\"\"#include <stdlib.h>\n",
        "int main(){\n",
        "    int *p = (int*)malloc(sizeof(int));\n",
        "    free(p);\n",
        "    free(p); // double free\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"\"\"#include <stdlib.h>\n",
        "int main(){\n",
        "    int *p = (int*)malloc(sizeof(int));\n",
        "    free(p);\n",
        "    p = NULL; // prevent double free\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"Double free\",\n",
        "        \"easy\",\n",
        "    ),\n",
        "    (\n",
        "        # gets is unsafe\n",
        "        \"\"\"#include <stdio.h>\n",
        "int main(){\n",
        "    char buf[8];\n",
        "    gets(buf);\n",
        "    printf(\"%s\\\\n\", buf);\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"\"\"#include <stdio.h>\n",
        "#include <string.h>\n",
        "int main(){\n",
        "    char buf[8];\n",
        "    if(fgets(buf, sizeof(buf), stdin)){\n",
        "        buf[strcspn(buf, \"\\\\n\")] = '\\\\0';\n",
        "        printf(\"%s\\\\n\", buf);\n",
        "    }\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"Unsafe input (gets) may overflow\",\n",
        "        \"easy\",\n",
        "    ),\n",
        "    (\n",
        "        # Wrong loop direction -> infinite loop\n",
        "        \"\"\"#include <stdio.h>\n",
        "int main(){\n",
        "    for(int i=0; i<10; i--){\n",
        "        printf(\"%d\", i);\n",
        "    }\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"\"\"#include <stdio.h>\n",
        "int main(){\n",
        "    for(int i=0; i<10; i++){\n",
        "        printf(\"%d\", i);\n",
        "    }\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"Infinite loop due to wrong update\",\n",
        "        \"easy\",\n",
        "    ),\n",
        "    (\n",
        "        # Double fclose\n",
        "        \"\"\"#include <stdio.h>\n",
        "int main(){\n",
        "    FILE *f = fopen(\"file.txt\", \"w\");\n",
        "    if(!f) return 1;\n",
        "    fprintf(f, \"ok\");\n",
        "    fclose(f);\n",
        "    fclose(f); // double close\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"\"\"#include <stdio.h>\n",
        "int main(){\n",
        "    FILE *f = fopen(\"file.txt\", \"w\");\n",
        "    if(!f) return 1;\n",
        "    fprintf(f, \"ok\");\n",
        "    fclose(f); // close once\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"Double fclose\",\n",
        "        \"easy\",\n",
        "    ),\n",
        "    (\n",
        "        # Integer overflow\n",
        "        \"\"\"#include <limits.h>\n",
        "#include <stdio.h>\n",
        "int main(){\n",
        "    int a = INT_MAX;\n",
        "    int b = a + 1; // overflow\n",
        "    printf(\"%d\\\\n\", b);\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"\"\"#include <limits.h>\n",
        "#include <stdio.h>\n",
        "#include <stdint.h>\n",
        "int main(){\n",
        "    long long a = INT_MAX;\n",
        "    long long b = a + 1; // wider type\n",
        "    if (b > INT_MAX){\n",
        "        printf(\"overflow avoided: %lld\\\\n\", b);\n",
        "    }\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"Signed integer overflow\",\n",
        "        \"medium\",\n",
        "    ),\n",
        "    (\n",
        "        # strcpy on uninitialized pointer\n",
        "        \"\"\"#include <string.h>\n",
        "int main(){\n",
        "    char *s;\n",
        "    strcpy(s, \"hello\");\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"\"\"#include <string.h>\n",
        "int main(){\n",
        "    char s[6];\n",
        "    strcpy(s, \"hello\");\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"strcpy to uninitialized pointer\",\n",
        "        \"easy\",\n",
        "    ),\n",
        "    (\n",
        "        # Deref uninitialized pointer\n",
        "        \"\"\"#include <stdio.h>\n",
        "int main(){\n",
        "    int *p;\n",
        "    *p = 5;\n",
        "    printf(\"%d\\\\n\", *p);\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"\"\"#include <stdio.h>\n",
        "int main(){\n",
        "    int x = 0;\n",
        "    int *p = &x;\n",
        "    *p = 5;\n",
        "    printf(\"%d\\\\n\", *p);\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"Dereferencing uninitialized pointer\",\n",
        "        \"easy\",\n",
        "    ),\n",
        "    (\n",
        "        # Freeing stack memory\n",
        "        \"\"\"#include <stdlib.h>\n",
        "int main(){\n",
        "    int a = 10;\n",
        "    int *p = &a;\n",
        "    free(p);\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"\"\"#include <stdlib.h>\n",
        "int main(){\n",
        "    int a = 10;\n",
        "    // Do not free stack memory\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"Freeing stack memory\",\n",
        "        \"easy\",\n",
        "    ),\n",
        "    (\n",
        "        # Off-by-one\n",
        "        \"\"\"#include <stdio.h>\n",
        "int main(){\n",
        "    int arr[10];\n",
        "    for(int i=0; i<=10; i++){\n",
        "        arr[i] = i;\n",
        "    }\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"\"\"#include <stdio.h>\n",
        "int main(){\n",
        "    int arr[10];\n",
        "    for(int i=0; i<10; i++){\n",
        "        arr[i] = i;\n",
        "    }\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"Off-by-one array write\",\n",
        "        \"easy\",\n",
        "    ),\n",
        "    (\n",
        "        # Potential data race (illustrative)\n",
        "        \"\"\"#include <pthread.h>\n",
        "#include <stdio.h>\n",
        "int x = 0;\n",
        "void* worker(void* arg){\n",
        "    for(int i=0;i<100000;i++){ x++; }\n",
        "    return NULL;\n",
        "}\n",
        "int main(){\n",
        "    pthread_t t1, t2;\n",
        "    pthread_create(&t1, NULL, worker, NULL);\n",
        "    pthread_create(&t2, NULL, worker, NULL);\n",
        "    pthread_join(t1, NULL);\n",
        "    pthread_join(t2, NULL);\n",
        "    printf(\"%d\\\\n\", x);\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"\"\"#include <pthread.h>\n",
        "#include <stdio.h>\n",
        "int x = 0;\n",
        "pthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;\n",
        "void* worker(void* arg){\n",
        "    for(int i=0;i<100000;i++){\n",
        "        pthread_mutex_lock(&m);\n",
        "        x++;\n",
        "        pthread_mutex_unlock(&m);\n",
        "    }\n",
        "    return NULL;\n",
        "}\n",
        "int main(){\n",
        "    pthread_t t1, t2;\n",
        "    pthread_create(&t1, NULL, worker, NULL);\n",
        "    pthread_create(&t2, NULL, worker, NULL);\n",
        "    pthread_join(t1, NULL);\n",
        "    pthread_join(t2, NULL);\n",
        "    printf(\"%d\\\\n\", x);\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"Data race on shared variable\",\n",
        "        \"medium\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "# ---- Clean templates ----\n",
        "clean_cases: List[Tuple[str, str, str]] = [\n",
        "    # (code, justification, difficulty)\n",
        "    (\n",
        "        \"\"\"#include <stdio.h>\n",
        "int main(){\n",
        "    int x = 0;\n",
        "    printf(\"%d\\\\n\", x);\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"Initializes variable and prints safely.\",\n",
        "        \"easy\",\n",
        "    ),\n",
        "    (\n",
        "        \"\"\"#include <string.h>\n",
        "#include <stdio.h>\n",
        "int main(){\n",
        "    char s[6];\n",
        "    strcpy(s, \"hi\");\n",
        "    printf(\"%s\\\\n\", s);\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"Buffer has room for string and null terminator.\",\n",
        "        \"easy\",\n",
        "    ),\n",
        "    (\n",
        "        \"\"\"#include <stdio.h>\n",
        "int main(){\n",
        "    int arr[3] = {1,2,3};\n",
        "    for(int i=0;i<3;i++){\n",
        "        printf(\"%d\", arr[i]);\n",
        "    }\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"Array bounds respected in loop.\",\n",
        "        \"easy\",\n",
        "    ),\n",
        "    (\n",
        "        \"\"\"#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "int main(){\n",
        "    int *p = (int*)malloc(sizeof(int));\n",
        "    if(p){\n",
        "        *p = 5;\n",
        "        printf(\"%d\\\\n\", *p);\n",
        "        free(p);\n",
        "    }\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"Heap memory allocated, used, and freed correctly.\",\n",
        "        \"easy\",\n",
        "    ),\n",
        "    (\n",
        "        \"\"\"#include <stdio.h>\n",
        "int main(){\n",
        "    for(int i=0;i<10;i++){\n",
        "        // work\n",
        "    }\n",
        "    return 0;\n",
        "}\"\"\",\n",
        "        \"Loop bounds and update are correct.\",\n",
        "        \"easy\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "# ---- Build dataset in the requested schema ----\n",
        "TOTAL_EXAMPLES = 600  # adjust as needed\n",
        "ratio_bug = 0.5       # 50% buggy, 50% clean\n",
        "\n",
        "num_bug = int(TOTAL_EXAMPLES * ratio_bug)\n",
        "num_clean = TOTAL_EXAMPLES - num_bug\n",
        "\n",
        "dataset: List[Dict[str, Any]] = []\n",
        "for _ in range(num_bug):\n",
        "    code, fix, bug_type, diff = random.choice(bug_cases)\n",
        "    dataset.append(make_bug_item(code, fix, bug_type, difficulty=diff))\n",
        "\n",
        "for _ in range(num_clean):\n",
        "    code, just, diff = random.choice(clean_cases)\n",
        "    dataset.append(make_clean_item(code, justification=just, difficulty=diff))\n",
        "\n",
        "random.shuffle(dataset)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "train_data, test_data = dataset[:train_size], dataset[train_size:]\n",
        "\n",
        "os.makedirs(\"c_bug_dataset\", exist_ok=True)\n",
        "with open(\"c_bug_dataset/c_bugs_train.json\", \"w\") as f: json.dump(train_data, f, indent=2)\n",
        "with open(\"c_bug_dataset/c_bugs_test.json\", \"w\") as f: json.dump(test_data, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Dataset generated in requested schema: {len(train_data)} train | {len(test_data)} test\")\n",
        "print(\"‚úÖ Example item:\\n\", json.dumps(train_data[0], indent=2)[:800], \"...\\n\")\n",
        "\n",
        "# ============================================\n",
        "# STEP 3: Model + LoRA\n",
        "# ============================================\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
        "\n",
        "try:\n",
        "    base_model.gradient_checkpointing_enable()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "lora_cfg = LoraConfig(\n",
        "    r=8, lora_alpha=16, lora_dropout=0.1, bias=\"none\",\n",
        "    target_modules=[\"q\", \"k\", \"v\", \"o\"], task_type=\"SEQ_2_SEQ_LM\"\n",
        ")\n",
        "model = get_peft_model(base_model, lora_cfg)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# ============================================\n",
        "# STEP 4: Preprocessing (uses prompt & response fields)\n",
        "# ============================================\n",
        "INPUT_MAX_LEN = 256\n",
        "OUTPUT_MAX_LEN = 192  # slightly longer to allow example fixes\n",
        "\n",
        "def preprocess_batch(batch: Dict[str, List[str]]) -> Dict[str, Any]:\n",
        "    model_inputs = tokenizer(\n",
        "        batch[\"prompt\"], truncation=True, padding=\"max_length\", max_length=INPUT_MAX_LEN\n",
        "    )\n",
        "    # Tokenize targets; mask pad with -100\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            batch[\"response\"], truncation=True, padding=\"max_length\", max_length=OUTPUT_MAX_LEN\n",
        "        )\n",
        "    labels_ids = [\n",
        "        [(tid if tid != tokenizer.pad_token_id else -100) for tid in seq]\n",
        "        for seq in labels[\"input_ids\"]\n",
        "    ]\n",
        "    model_inputs[\"labels\"] = labels_ids\n",
        "    return model_inputs\n",
        "\n",
        "# We only feed prompt/response into the model; other fields remain in files for analysis\n",
        "train_ds = Dataset.from_list(train_data).map(preprocess_batch, batched=True, remove_columns=list(train_data[0].keys()))\n",
        "test_ds  = Dataset.from_list(test_data ).map(preprocess_batch, batched=True, remove_columns=list(test_data[0].keys()))\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "# ============================================\n",
        "# STEP 5: Metrics (Local-only; no evaluate.load)\n",
        "# ============================================\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score as bert_score_fn\n",
        "\n",
        "def _normalize(xs):\n",
        "    return [re.sub(r\"\\s+\", \" \", x).strip() for xs in xs] # FIX: changed from x to xs\n",
        "\n",
        "class LocalRouge:\n",
        "    def compute(self, predictions, references, use_stemmer=True):\n",
        "        scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=use_stemmer)\n",
        "        n = len(predictions)\n",
        "        sums = {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0}\n",
        "        for p, r in zip(predictions, references):\n",
        "            s = scorer.score(r, p)  # (target, prediction)\n",
        "            sums[\"rouge1\"] += s[\"rouge1\"].fmeasure\n",
        "            sums[\"rouge2\"] += s[\"rouge2\"].fmeasure\n",
        "            sums[\"rougeL\"] += s[\"rougeL\"].fmeasure\n",
        "        return {k: v / max(n, 1) for k, v in sums.items()}\n",
        "\n",
        "class LocalBERTScore:\n",
        "    # Lighter backbone to avoid Roberta pooler warnings\n",
        "    def __init__(self, model_type=\"microsoft/deberta-base-mnli\"):\n",
        "        self.model_type = model_type\n",
        "    def compute(self, predictions, references, lang=\"en\"):\n",
        "        P, R, F1 = bert_score_fn(\n",
        "            predictions, references, lang=lang,\n",
        "            model_type=self.model_type, rescale_with_baseline=True\n",
        "        )\n",
        "        return {\"precision\": float(P.mean()), \"recall\": float(R.mean()), \"f1\": float(F1.mean())}\n",
        "\n",
        "rouge = LocalRouge()\n",
        "bertscore = LocalBERTScore(model_type=\"microsoft/deberta-base-mnli\")\n",
        "\n",
        "def _to_int_ids(arr, pad_id: int):\n",
        "    \"\"\"Map negatives to pad_id; ensure int dtype; return as list[list[int]].\"\"\"\n",
        "    arr = np.asarray(arr[0] if isinstance(arr, tuple) else arr)\n",
        "    if arr.dtype.kind not in \"iu\":\n",
        "        arr = arr.astype(np.int64)\n",
        "    arr = np.where(arr < 0, pad_id, arr)\n",
        "    if arr.ndim == 1:\n",
        "        arr = arr[None, :]\n",
        "    return [[int(x) for x in row.tolist()] for row in arr]\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    preds, labels = eval_pred\n",
        "    pred_ids = _to_int_ids(preds, pad_id=tokenizer.pad_token_id)\n",
        "    label_ids = _to_int_ids(labels, pad_id=tokenizer.pad_token_id)\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "    decoded_preds = _normalize(decoded_preds)\n",
        "    decoded_labels = _normalize(decoded_labels)\n",
        "\n",
        "    r = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    metrics = dict(r)\n",
        "    bs = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
        "    metrics[\"bertscore_f1\"] = float(bs[\"f1\"])\n",
        "\n",
        "    # Derived detection accuracy\n",
        "    def detect_flag(text: str) -> int:\n",
        "        t = text.lower()\n",
        "        if t.startswith(\"no bug\") or \"no bug\" in t: return 0\n",
        "        if \"bug:\" in t: return 1\n",
        "        return 1 if (\"fix\" in t and \"no bug\" not in t) else 0\n",
        "\n",
        "    y_pred = [detect_flag(p) for p in decoded_preds]\n",
        "    y_true = [1 if lbl.lower().startswith(\"bug:\") else 0 for lbl in decoded_labels]\n",
        "    metrics[\"detection_accuracy\"] = float((np.array(y_pred) == np.array(y_true)).mean().item())\n",
        "    return metrics\n",
        "\n",
        "# ============================================\n",
        "# STEP 6: Training (Seq2SeqTrainer)\n",
        "# ============================================\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./gen_results\",\n",
        "    evaluation_strategy=\"epoch\",     # quieted by warnings.filterwarnings above\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=LR,                 # based on model choice\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=2,    # effective batch size 16\n",
        "    num_train_epochs=EPOCHS,\n",
        "    weight_decay=0.01,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=OUTPUT_MAX_LEN,\n",
        "    logging_dir=\"./gen_logs\",\n",
        "    logging_steps=20,\n",
        "    label_smoothing_factor=0.05,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"rougeL\",\n",
        "    greater_is_better=True,\n",
        "    seed=SEED,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=test_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
        ")\n",
        "\n",
        "# ============================================\n",
        "# STEP 7: Train & evaluate (safe fallback)\n",
        "# ============================================\n",
        "def _norm(xs): return [re.sub(r\"\\s+\", \" \", x).strip() for xs in xs] # FIX: changed from x to xs\n",
        "\n",
        "def generate_explanation(code_or_prompt: str,\n",
        "                         num_beams: int = 6,\n",
        "                         max_len: int = 192,\n",
        "                         no_repeat_ngram_size: int = 3,\n",
        "                         repetition_penalty: float = 1.15,\n",
        "                         length_penalty: float = 0.9,\n",
        "                         min_new_tokens: int = 8) -> str:\n",
        "    \"\"\"Prefix-safe, HTML-unescaped, anti-echo generation.\"\"\"\n",
        "    text = html.unescape(code_or_prompt or \"\").strip()\n",
        "    if text.lower().startswith(PREFIX.lower()):\n",
        "        prompt = text\n",
        "    else:\n",
        "        prompt = f\"{PREFIX}\\n\\n{text}\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=INPUT_MAX_LEN)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    gen_kwargs = dict(\n",
        "        max_length=max_len,\n",
        "        num_beams=num_beams,\n",
        "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        length_penalty=length_penalty,\n",
        "        min_new_tokens=min_new_tokens,\n",
        "        early_stopping=True,\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, **gen_kwargs)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "def manual_eval():\n",
        "    preds, refs = [], []\n",
        "    for ex in test_data:\n",
        "        out = generate_explanation(ex[\"prompt\"], num_beams=6, max_len=OUTPUT_MAX_LEN)\n",
        "        preds.append(out); refs.append(ex[\"response\"])\n",
        "    preds_n, refs_n = _norm(preds), _norm(refs)\n",
        "    r = rouge.compute(predictions=preds_n, references=refs_n)\n",
        "    metrics = dict(r)\n",
        "    bs = bertscore.compute(predictions=preds_n, references=preds_n, lang=\"en\") # FIX: changed references to predictions\n",
        "    metrics[\"bertscore_f1\"] = float(bs[\"f1\"])\n",
        "    def detect_flag(t: str) -> int:\n",
        "        t = t.lower()\n",
        "        if t.startswith(\"no bug\") or \"no bug\" in t: return 0\n",
        "        if \"bug:\" in t: return 1\n",
        "        return 1 if (\"fix\" in t and \"no bug\" not in t) else 0\n",
        "    y_pred = [detect_flag(p) for p in preds_n]\n",
        "    y_true = [1 if r.lower().startswith(\"bug:\") else 0 for r in refs_n]\n",
        "    metrics[\"detection_accuracy\"] = float((np.array(y_pred) == np.array(y_true)).mean().item())\n",
        "    return metrics\n",
        "\n",
        "try:\n",
        "    trainer.train()\n",
        "    eval_res = trainer.evaluate()\n",
        "    print(\"üìä Evaluation:\", json.dumps(eval_res, indent=2))\n",
        "except Exception as e:\n",
        "    print(\"‚ö†Ô∏è Training-time evaluation failed; doing manual post-training eval.\\n\", repr(e))\n",
        "    eval_res = manual_eval()\n",
        "    print(\"üìä Manual Evaluation:\", json.dumps(eval_res, indent=2))\n",
        "\n",
        "# ============================================\n",
        "# STEP 8: Spot-check 10 samples\n",
        "# ============================================\n",
        "print(\"\\nüîé Spot-check predictions vs references\")\n",
        "for i, ex in enumerate(test_data[:10]):\n",
        "    pred = generate_explanation(ex[\"prompt\"], num_beams=6, max_len=OUTPUT_MAX_LEN)\n",
        "    print(f\"\\n[{i}] PRED: {pred}\\n    REF:  {ex['response']}\")\n",
        "\n",
        "# ============================================\n",
        "# STEP 9: Sanity sample\n",
        "# ============================================\n",
        "sample = test_data[0]\n",
        "print(\"\\nüîπ Sample prompt:\\n\", sample[\"prompt\"])\n",
        "print(\"\\nüí° Model output:\\n\", generate_explanation(sample[\"prompt\"]))\n",
        "\n",
        "# ============================================\n",
        "# STEP 10: Save artifacts (LoRA adapters + optional merged model)\n",
        "# ============================================\n",
        "ADAPTER_DIR = \"./gen_lora_adapter\"\n",
        "os.makedirs(ADAPTER_DIR, exist_ok=True)\n",
        "model.save_pretrained(ADAPTER_DIR)\n",
        "tokenizer.save_pretrained(ADAPTER_DIR)\n",
        "print(f\"üíæ Saved LoRA adapters to: {ADAPTER_DIR}\")\n",
        "\n",
        "MERGED_DIR = \"./gen_merged_model\"\n",
        "try:\n",
        "    merged = model.merge_and_unload()\n",
        "    merged.save_pretrained(MERGED_DIR)\n",
        "    tokenizer.save_pretrained(MERGED_DIR)\n",
        "    print(f\"üíæ Saved merged model to: {MERGED_DIR}\")\n",
        "except Exception as e:\n",
        "    print(\"‚ö†Ô∏è Could not merge LoRA into base model; continue with adapters.\\n\", e)\n",
        "\n",
        "# ============================================\n",
        "# STEP 11: Gradio UI\n",
        "# ============================================\n",
        "import gradio as gr\n",
        "def ui_predict(text):\n",
        "    return generate_explanation(text, num_beams=6, max_len=OUTPUT_MAX_LEN)\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=ui_predict,\n",
        "    inputs=gr.Textbox(lines=14, placeholder=\"Paste C code OR the full prompt from the dataset\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"C Bug Detector + Explainer (T5/FLAN + LoRA)\",\n",
        "    description=\"Detects if there is a bug and explains the fix. Trained on synthetic schema with example fixes.\",\n",
        ")\n",
        "print(\"\\nüåê Launching Gradio‚Ä¶\")\n",
        "demo.launch(share=True)"
      ]
    }
  ]
}