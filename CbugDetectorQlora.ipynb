{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIWbkQz2a4FFGqOqeYYCsn"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-qDPwI-98eH"
      },
      "outputs": [],
      "source": [
        "# ===========================\n",
        "# Script 2: Train with QLoRA\n",
        "# ===========================\n",
        "!pip install transformers peft accelerate bitsandbytes datasets -q\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch\n",
        "\n",
        "# --------------------------\n",
        "# LOAD MODEL\n",
        "# --------------------------\n",
        "model_name = \"codellama/CodeLlama-7b-hf\"  # or try starcoder\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# --------------------------\n",
        "# LOAD DATASET\n",
        "# --------------------------\n",
        "dataset = load_dataset(\"json\", data_files=\"synthetic_c_bugs.json\")\n",
        "\n",
        "# Create input text with classification label + explanation\n",
        "def format_example(example):\n",
        "    bug_label = \"BUG\" if example[\"label\"]==1 else \"NO_BUG\"\n",
        "    return f\"Prompt: {example['prompt']}\\nLabel: {bug_label}\\nExplanation: {example['response']}\"\n",
        "\n",
        "def tokenize_function(example):\n",
        "    text = format_example(example)\n",
        "    return tokenizer(text, truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "tokenized = dataset.map(tokenize_function, batched=False)\n",
        "\n",
        "# --------------------------\n",
        "# LORA CONFIG\n",
        "# --------------------------\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\",\"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# --------------------------\n",
        "# TRAINING CONFIG\n",
        "# --------------------------\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=1,\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized[\"train\"],\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "# --------------------------\n",
        "# SAVE MODEL\n",
        "# --------------------------\n",
        "model.save_pretrained(\"c_bug_lora\")\n",
        "print(\"âœ… Model saved as c_bug_lora\")\n",
        "\n",
        "# --------------------------\n",
        "# TESTING PIPELINE\n",
        "# --------------------------\n",
        "from transformers import pipeline\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
        "\n",
        "test_prompt = \"\"\"Find bug in this C code:\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "int main() {\n",
        "  int *p = malloc(5 * sizeof(int));\n",
        "  for(int i=0;i<=5;i++) p[i]=i; // potential bug?\n",
        "  return 0;\n",
        "}\"\"\"\n",
        "\n",
        "print(pipe(test_prompt, max_length=300, do_sample=True)[0][\"generated_text\"])\n"
      ]
    }
  ]
}